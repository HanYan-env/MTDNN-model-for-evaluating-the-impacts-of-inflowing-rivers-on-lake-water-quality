file_path = r''
data = pd.read_excel(file_path)
feature_columns = list(range(0, 37))
output_columns = list(range(37, 45))
cols_to_use = [date_column_index] + feature_columns + output_columns
data = pd.read_excel(file_path, usecols=cols_to_use)
data = data.iloc[1:]
data.fillna(method='ffill', inplace=True)
features = data.iloc[:, :-len(output_columns)]
targets = data.iloc[:, -len(output_columns):]
scaler_features = StandardScaler()
scaled_features = pd.DataFrame(scaler_features.fit_transform(features))
scaler_targets = StandardScaler()  
targets_scaled = pd.DataFrame(scaler_targets.fit_transform(targets))
#X_train, X_test, y_train, y_test = train_test_split(scaled_features, targets_scaled, test_size=0.2, random_state=42)
split_point = int(len(data) * 0.8)
X_train = scaled_features.iloc[:split_point]
y_train = targets_scaled.iloc[:split_point]
X_test = scaled_features.iloc[split_point:]
y_test = targets_scaled.iloc[split_point:]
data['Date'] = pd.to_datetime(data['Date'])
dates = data['Date']
train_dates = dates[:split_point]
test_dates = dates[split_point:]
input_shape = X_train.shape[1]  
input_layer = Input(shape=(input_shape,))
dense_layer_1 = Dense(64, activation='relu')(input_layer)
dropout_1 = Dropout(0.5)(dense_layer_1)
dense_layer_2 = Dense(64, activation='relu')(dropout_1)
output_layers = [Dense(1, activation='linear', name=f'output_{i+1}')(dense_layer_2) for i in range(y_train.shape[1])]
model = Model(inputs=input_layer, outputs=output_layers)
model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mae'])
model.summary()
train_targets = np.split(y_train, y_train.shape[1], axis=1)
test_targets = np.split(y_test, y_test.shape[1], axis=1)
history = model.fit(X_train, train_targets, epochs=100, batch_size=32, validation_data=(X_test, test_targets))
model.save('my_trained_model.keras')

#shap analyses#
explainer = shap.KernelExplainer(model_predict, X_train_summary)
shap_values = explainer.shap_values(X_test, nsamples=xx)
